{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/Users/chinmayshrivastava/Desktop/OSCA/V4.0/scraping/crawler')\n",
    "from generalscraper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'blockchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://builtin.com/blockchain',\n",
       " 'https://en.wikipedia.org/wiki/Blockchain',\n",
       " 'https://www.coinbase.com/learn/crypto-basics/what-is-a-blockchain',\n",
       " 'https://www.euromoney.com/learning/blockchain-explained/what-is-blockchain',\n",
       " 'https://www.forbes.com/advisor/investing/cryptocurrency/what-is-blockchain/',\n",
       " 'https://www.ibm.com/topics/what-is-blockchain',\n",
       " 'https://www.investopedia.com/terms/b/blockchain.asp',\n",
       " 'https://www.pwc.com/us/en/industries/financial-services/fintech/bitcoin-blockchain-cryptocurrency.html',\n",
       " 'https://www.simplilearn.com/tutorials/blockchain-tutorial/blockchain-technology',\n",
       " 'https://www.synopsys.com/glossary/what-is-blockchain.html'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getgooglelinks(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.synopsys.com/glossary/what-is-blockchain.html'\n",
    "database, headings, paragraphs = articlescraper(url, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = getpage(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body class=\"page publish\" data-content-type=\"eda\" data-sticky-nav=\"false\" data-template=\"content-page-template\">\n",
      "<script>\n",
      " var camp=document.getElementById('campaign').value;\n",
      "               url = new URL(window.location.href);\n",
      "if (url.searchParams.get('intcmp') || camp.length>2 ) {\n",
      "const params = new Proxy(new URLSearchParams(window.location.search), {\n",
      "  \t\t\tget: (searchParams, prop) => searchParams.get(prop),\n",
      "\t\t\t});\n",
      "\t\t\tcampaignCode = params.intcmp;\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " window.adobeDataLayer = window.adobeDataLayer || [];\n",
      "          window.adobeDataLayer.push({\n",
      "              web: {\n",
      "              webPageDetails: {\n",
      "\t\t\t\t\tURL:'https:\\/\\/www.synopsys.com\\/glossary\\/what\\u002Dis\\u002Dblockchain.html',\n",
      "            \t\tname:'Blockchain',\n",
      "\n",
      "              }\n",
      "              },\n",
      "              _synopsys:{\n",
      "                         internalCampaign: {\n",
      "             campaignCode: campaignCode,\n",
      "             campaignName: '',\n",
      "             campaignClicks: {value: 1},\n",
      "\n",
      "                   }\n",
      "                      }\n",
      "               });}\n",
      "               url2 = new URL(window.location.href);\n",
      "               if (url2.searchParams.get('cmp') || url2.searchParams.get('utm_source') || url2.searchParams.get('utm_medium') || url2.searchParams.get('utm_campaign') || url2.searchParams.get('utm_group')||  url2.searchParams.get('utm_creative')||  url2.searchParams.get('utm_keyword'))   {\n",
      "                var utm_source='no_source';\n",
      "                                var utm_cmp='no_cmp';\n",
      "\n",
      "                var utm_medium='no_medium';\n",
      "                var utm_campaign='no_campaign';\n",
      "                var utm_group='no_group';\n",
      "                var utm_creative='no_creative';\n",
      "                var utm_keyword='no_keyword';\n",
      "                 const params = new Proxy(new URLSearchParams(window.location.search), {\n",
      "  \t\t\t\t\tget: (searchParams, prop) => searchParams.get(prop),\n",
      "\t\t\t\t\t});\n",
      "                               if(params.utm_source!=null)\n",
      "                               utm_source = params.utm_source;\n",
      "                                if(params.utm_medium!=null)\n",
      "\t\t\t\t\t\t\t   utm_medium  = params.utm_medium;\n",
      "                                if(params.utm_campaign!=null)\n",
      "\t\t\t\t\t\t\t   utm_campaign  = params.utm_campaign;\n",
      "                               if(params.utm_group!=null)\n",
      "\t\t\t\t\t\t\t   utm_group  = params.utm_group;\n",
      "                               if(params.utm_creative!=null)\n",
      "\t\t\t\t\t\t\t   utm_creative  = params.utm_creative;\n",
      "                               if(params.utm_keyword!=null)\n",
      "\t\t\t\t\t\t\t   utm_keyword  = params.utm_keyword;\n",
      "                               if(params.cmp!=null)\n",
      "                               {\n",
      "                               utm_cmp = params.cmp;\n",
      "                               var trackingCode=utm_cmp;\n",
      "                               }\n",
      "                               else\n",
      "                               {\n",
      "                               var trackingCode=utm_source+\":\"+utm_medium+\":\"+utm_campaign+\":\"+utm_group+\":\"+utm_creative+\":\"+utm_keyword;\n",
      "                               }\n",
      "\n",
      "                                window.adobeDataLayer = window.adobeDataLayer || [];\n",
      "          \t\t\t\t\t\twindow.adobeDataLayer.push({\n",
      "              web: {\n",
      "              webPageDetails: {\n",
      "\t\t\t\t\tURL:'https:\\/\\/www.synopsys.com\\/glossary\\/what\\u002Dis\\u002Dblockchain.html',\n",
      "            \t\tname:'Blockchain',\n",
      "\n",
      "              }\n",
      "              },\n",
      "              _synopsys:{\n",
      "                         externalCampaign: {\n",
      "\n",
      "             campaignClicks: {value: 1},\n",
      "\n",
      "                   }\n",
      "                      },\n",
      "                              marketing:{\n",
      "                         trackingCode: trackingCode,\n",
      "\n",
      "                   }\n",
      "               });\n",
      "               }\n",
      " let hs_id='';\n",
      "const params2 = new Proxy(new URLSearchParams(window.location.search), {\n",
      "  get: (searchParams, prop) => searchParams.get(prop),\n",
      "});\n",
      "     hs_id = params2.hs_id;\n",
      "\n",
      "         window.adobeDataLayer = window.adobeDataLayer || [];\n",
      "          window.adobeDataLayer.push({\n",
      "              event: \"page-view\",\n",
      "              web: {\n",
      "              webPageDetails: {\n",
      "              name: 'Blockchain',\n",
      "              url: 'https:\\/\\/www.synopsys.com\\/glossary\\/what\\u002Dis\\u002Dblockchain.html',\n",
      "              siteSection: 'Glossary',\n",
      "              server: window.location.hostname,\n",
      "              pageViews: {\n",
      "                         value: 1\n",
      "\n",
      "                   },\n",
      "              }\n",
      "              },\n",
      "              _synopsys:{\n",
      "                 web:{\n",
      "                    webPageDetails:{\n",
      "                      webURL: window.location.href,\n",
      "                      pageType: 'Content',\n",
      "                      siteSubSection2: 'Glossary|Blockchain',\n",
      "                      siteSubSection3: 'n\\/a',\n",
      "                      siteSubSection4: 'n\\/a',\n",
      "                      siteSubSection5: 'n\\/a',\n",
      "                      siteName: 'synopsys',\n",
      "                      countryLanguage: 'en_US',\n",
      "                      articleAuthor: '',\n",
      "                      pageTitle: 'Blockchain',\n",
      "\n",
      "                      defensicsProtocol: '',\n",
      "                      defensicsIndustry: '',\n",
      "                      defensicsTechnology: ''\n",
      "                   },\n",
      "                   queryParam: {\n",
      "                      hsid: hs_id\n",
      "                   }\n",
      "\n",
      "\n",
      "                  }\n",
      "               }\n",
      "            });\n",
      "\n",
      "     </script>\n",
      "<div class=\"site-wrapper\">\n",
      "<div class=\"root synopsysContainer responsivegrid\">\n",
      "<div class=\"aem-Grid aem-Grid--12 aem-Grid--default--12\">\n",
      "<div class=\"experiencefragment aem-GridColumn aem-GridColumn--default--12\">\n",
      "<div class=\"cmp-experiencefragment cmp-experiencefragment--topnav\" id=\"experiencefragment-212fa494e2\">\n",
      "<div class=\"xf-content-height\">\n",
      "<div class=\"aem-Grid aem-Grid--12 aem-Grid--default--12\">\n",
      "<div class=\"topNav image aem-GridColumn aem-GridColumn--default--12\">\n",
      "<div class=\"component-nav-top\" id=\"topNav\">\n",
      "<div class=\"nav-top-wrapper\">\n",
      "<div class=\"nav-container clearfix\">\n",
      "<div class=\"nav-brand\">\n",
      "<a href=\"https://www.synopsys.com\">\n",
      "<img alt=\"Synopsys Home Page\" class=\"logo\" src=\"/content/experience-fragments/synopsys/en-us/global/eda/topnav/master/_jcr_content/root/topnav_copy.coreimg.svg/1646177682209/synopsys-logo-color.svg\" style=\"background: none;\"/>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"nav-go-back-wrapper\">\n",
      "<a href=\"#\" id=\"mobile-nav-go-back\">Go Back</a>\n",
      "</div>\n",
      "<div class=\"nav-items-center hidden-xs\">\n",
      "<ul>\n",
      "<li data-menu=\"Solutions\">\n",
      "<a href=\"#\">Solutions</a>\n",
      "</li>\n",
      "<li data-menu=\"Products\">\n",
      "<a href=\"#\">Products</a>\n",
      "</li>\n",
      "<li data-menu=\"Support\">\n",
      "<a href=\"#\">Support</a>\n",
      "</li>\n",
      "<li data-menu=\"Company\">\n",
      "<a href=\"#\">Company</a>\n",
      "</li>\n",
      "</ul>\n",
      "</div>\n",
      "<div class=\"nav-items-right\">\n",
      "<ul>\n",
      "<li class=\"right-nav-link\" id=\"nav-coveo-search-btn\">\n",
      "<a href=\"\">\n",
      "<span class=\"visually-hidden\">Search Synopsys.com</span>\n",
      "<div class=\"icon-search\"></div>\n",
      "</a>\n",
      "</li>\n",
      "<li class=\"right-nav-link\" id=\"nav-global-sites-btn\">\n",
      "<a href=\"\">\n",
      "<span class=\"visually-hidden\">Global Sites</span>\n",
      "<div class=\"icon-globe\"></div>\n",
      "</a>\n",
      "</li>\n",
      "<li class=\"right-nav-link\" id=\"nav-mobile-menu-btn\">\n",
      "<a href=\"\">\n",
      "<span class=\"visually-hidden\">Menu</span>\n",
      "<div class=\"icon-mobile-menu\"></div>\n",
      "</a>\n",
      "</li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"secondary-nav purple-bottom-border\" id=\"coveo-search-wrapper\">\n",
      "<div class=\"container\">\n",
      "<div class=\"row\">\n",
      "<div class=\"col-xs-12\">\n",
      "<section class=\"component-search clearfix\">\n",
      "<div class=\"snps-searchbox\" data-enable-history=\"true\" data-endpoint=\"default\" id=\"searchbox\">\n",
      "<div class=\"CoveoAnalytics\"></div>\n",
      "<script class=\"CoveoPipelineContext\"></script>\n",
      "<div class=\"CoveoEnhancedPipelineContext\"></div>\n",
      "<div class=\"coveo-tab-section\" style=\"display: none;\">\n",
      "<a class=\"CoveoTab\" data-caption=\"All Content\" data-id=\"All\"></a>\n",
      "</div>\n",
      "<div class=\"coveo-search-section\">\n",
      "<div class=\"CoveoSearchbox\" data-add-search-button=\"false\" data-placeholder=\"Search\" data-query-suggest-character-threshold=\"1\"></div>\n",
      "</div>\n",
      "</div>\n",
      "</section>\n",
      "<a class=\"close-button\" href=\"\">\n",
      "<div class=\"icon-caret-up\"></div>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"nav-left-wrapper\" id=\"global-sites-wrapper\">\n",
      "<div class=\"aem-Grid aem-Grid--12 aem-Grid--default--12\">\n",
      "<div class=\"subNavLinks image aem-GridColumn aem-GridColumn--default--12\">\n",
      "<div class=\"component-nav-left secondary-nav simplenav purple-bottom-border\">\n",
      "<div class=\"component-nav-left-top\">\n",
      "<div class=\"container\">\n",
      "<div class=\"row\">\n",
      "<!-- simple nav Markup-->\n",
      "<div class=\"col-xs-12\">\n",
      "<ul class=\"nav-left-dropdown\">\n",
      "<li class=\"nav-left-item spacer\" data-active=\"false\" data-link=\"/content/synopsys/ja-jp.html\">\n",
      "<a href=\"/ja-jp.html\">日本語</a></li></ul></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body>\n"
     ]
    }
   ],
   "source": [
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['topic', 'Blockchain', 1, None, [0]],\n",
       " ['h2', 'Definition', None, 2, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]],\n",
       " ['h2',\n",
       "  'What are the business benefits of blockchain?',\n",
       "  3,\n",
       "  4,\n",
       "  [11, 12, 13, 14]],\n",
       " ['h3',\n",
       "  'How to adapt software security best practices to blockchain',\n",
       "  None,\n",
       "  None,\n",
       "  [15]],\n",
       " ['h2',\n",
       "  'Blockchain explained',\n",
       "  None,\n",
       "  5,\n",
       "  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]],\n",
       " ['h2', 'Blockchain and Hyperledger', None, 6, [28, 29, 30]],\n",
       " ['h2', 'Blockchain security', None, 7, [31, 32, 33, 34, 35]],\n",
       " ['h2', 'Continue reading', 8, 12, []],\n",
       " ['h4', 'Synopsys Solutions for Application Security', None, 9, [36]],\n",
       " ['h4',\n",
       "  'Gartner Magic Quadrant for Application Security Testing',\n",
       "  None,\n",
       "  10,\n",
       "  [37]],\n",
       " ['h4', 'Software Risk is Business Risk', None, 11, [38]],\n",
       " ['h4', 'Ready to get started?', None, None, [39]],\n",
       " ['h2', 'Footer', 13, 21, []],\n",
       " ['h3', 'Corporate Headquarters', None, 14, [40]],\n",
       " ['h3', 'Customer Support', None, 15, [41]],\n",
       " ['h3', 'Worldwide Location', None, 16, [42]],\n",
       " ['h3', 'Products', None, 17, [43, 44, 45, 46, 47]],\n",
       " ['h3', 'Resources', None, 18, [48, 49, 50, 51, 52, 53]],\n",
       " ['h3', 'Corporate', None, 19, [54, 55, 56, 57, 58, 59]],\n",
       " ['h3', 'Legal', None, 20, [60, 61, 62]],\n",
       " ['h3', 'Follow', None, None, []],\n",
       " ['h2', 'Cookie Preference Center', 22, None, [63, 64, 65, 66, 67]],\n",
       " ['h3', 'Your Privacy', None, 23, []],\n",
       " ['h3', 'Strictly Necessary Cookies', None, 24, []],\n",
       " ['h3', 'Functional Cookies', None, 25, []],\n",
       " ['h3', 'Performance Cookies', None, 26, []],\n",
       " ['h3', 'Targeting Cookies', 27, 32, []],\n",
       " ['h4', 'Your Privacy', None, 28, [68]],\n",
       " ['h4', 'Strictly Necessary Cookies', None, 29, [69]],\n",
       " ['h4', 'Functional Cookies', None, 30, [70]],\n",
       " ['h4', 'Performance Cookies', None, 31, [71]],\n",
       " ['h4', 'Targeting Cookies', None, None, [72]],\n",
       " ['h3', 'Back', None, None, [73, 74, 75]]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['p', 'Economist and data scientist'],\n",
       " ['p',\n",
       "  'A while ago I was trying to perform an analysis of a Medium publication for a personal project. But getting the data was a problem – scraping only the publication’s home page does not guarantee that you get all the data you want.'],\n",
       " ['p',\n",
       "  'That’s when I found out that each publication has its own archive. You just have to type “/archive” after the publication URL. You can even specify a year, month, and day and find all the stories published on that date. Something like this:'],\n",
       " ['p',\n",
       "  'And suddenly the problem was solved. A very simple scraper did the job.'],\n",
       " ['p',\n",
       "  'In this tutorial, we’ll see how to code a simple but powerful web scraper that can be used to scrape any Medium publication.'],\n",
       " ['p',\n",
       "  'You can also use this scraper to scrape data from lots of different websites for whatever reason you want. Just make sure they allow scraping before you start.'],\n",
       " ['p',\n",
       "  \"Since we’re learning how to scrape a Medium publication, we'll use The Startup as an example. According to the publication, The Startup is the largest active Medium publication with over 700k followers. So it should be a great source of data.\"],\n",
       " ['p',\n",
       "  \"In this article, you’ll see how to scrape all the articles published by The Startup in 2019. We'll also look at how this data can be useful.\"],\n",
       " ['p',\n",
       "  'Web scraping is the process of collecting data from websites using automatized scripts. It consists of three main steps: fetching the page, parsing the HTML, and extracting the information you need.'],\n",
       " ['p',\n",
       "  'The third step is the one that can be a little tricky at first. It consists basically of finding the parts of the HTML that contain the information you want.'],\n",
       " ['p',\n",
       "  'You can find this info by opening the page you want to scrape and pressing the F12 key on your keyboard. Then you can select an element of the page to inspect. You can see this in the image below.'],\n",
       " ['p',\n",
       "  \"Then all you need to do is to use the tags and classes in the HTML to inform the scraper where to find the information. You need to do this for every part of the page you want to scrape. You can see it better in the code, so let's go there next.\"],\n",
       " ['p',\n",
       "  'As this is a simple scraper, we’ll only use requests, BeautifulSoup, and Pandas.'],\n",
       " ['p',\n",
       "  \"Requests will be used to get the pages we need, while BeautifulSoup parses the HTML. We'll use Pandas to store the data in a DataFrame and then export it as a .csv file.\"],\n",
       " ['p',\n",
       "  'So we’ll begin by importing these libraries and initializing an empty list to store the data. This list will be filled with other lists.'],\n",
       " ['p',\n",
       "  'As I mentioned earlier, the Medium archive stores the stories by the date of publication. As we want to scrape every story published in The Startup in 2019, we need to iterate over every day of every month of that year.'],\n",
       " ['p',\n",
       "  'We’ll use nested for loops to iterate over the months of the year and then over the days of each month.'],\n",
       " ['p',\n",
       "  'To do this, it is important to differentiate the number of days in each month. It’s also important to make sure all days and months are represented by two-digit numbers.'],\n",
       " ['p',\n",
       "  'And now the scraping begins. We can use the month and day to set up the date that will also be stored along with the scraped data and, of course, that creates the URL for that specific day.'],\n",
       " ['p',\n",
       "  'When this is done, we can just use requests to get the page and parse the HTML with BeautifulSoup.'],\n",
       " ['p',\n",
       "  'So this is The Startup’s archive page for January 1st, 2019. We can see that each story is stored in a container. What we need to do is to grab all these containers. To do this, we’ll use the find_all method.'],\n",
       " ['p',\n",
       "  'The above code generates a list containing all the story containers on the page. Something like this:'],\n",
       " ['p',\n",
       "  'All we need to do now is to iterate over it and grab the information we want from each story. We’ll scrape:'],\n",
       " ['oli',\n",
       "  'The author’s URL, from which we can later extract the author’s username if we want to'],\n",
       " ['oli', 'The reading time'],\n",
       " ['oli', 'The story title and subtitle'],\n",
       " ['oli', 'The number of claps and responses'],\n",
       " ['oli', 'The story URL from the Read more… button.'],\n",
       " ['p',\n",
       "  \"We’ll first select a box inside the container that I call the author’s box. From this box, we’ll extract the author's URL and the reading time.\"],\n",
       " ['p',\n",
       "  'And here is our only condition in this scraper: if the container does not show a reading time, we’ll not scrape this story and instead move to the next one. This is because such stories contain only images and one or two lines of text.'],\n",
       " ['p',\n",
       "  'We’re not interested in those as we can think of them as outliers. We’ll use the try and except blocks to handle those cases.'],\n",
       " ['p',\n",
       "  \"Other than that, we must be prepared if a story doesn't have a title or a subtitle (yes, that happens) and if it doesn't have claps or responses. The if clause will do the job of preventing an error from being raised in such situations.\"],\n",
       " ['p',\n",
       "  'All this scraped information will later be appended to the each_story list that is initialized in the loop. This is the code for all this:'],\n",
       " ['p',\n",
       "  'Before we move to scrape the text of the stories, let’s first do a little cleaning in the reading_time and responses data.'],\n",
       " ['p',\n",
       "  'Instead of storing these variables as “5 min read” and “5 responses”, we’ll keep only the numbers. These two lines of code will get this done:'],\n",
       " ['p',\n",
       "  \"We'll now scrape the article page. We’ll use requests once more to get the story_url page, and BeautifulSoup to parse the HTML.\"],\n",
       " ['p',\n",
       "  'From the article page, we need to find all the section tags, which are where the text of the article is located. We’ll also initialize two new lists, one to store the article’s paragraphs and the other to store the title of each section in the article.'],\n",
       " ['p',\n",
       "  'And now we only need to loop through the sections. For each section, we will:'],\n",
       " ['uli', 'Find all paragraphs and append them to the paragraphs list'],\n",
       " ['uli', 'Find all section titles and append them to the section titles list'],\n",
       " ['uli',\n",
       "  'Use these two lists to calculate the number of paragraphs and the number of sections in the article, as this could be some useful data to have.'],\n",
       " ['p',\n",
       "  'This will significantly increase the time it takes to scrape everything, but it will also make the final dataset much more valuable.'],\n",
       " ['p',\n",
       "  'The scraping is now finished. Everything will now be appended to the each_story list, which will be appended to the stories_data list.'],\n",
       " ['p',\n",
       "  'As stories_data is now a list of lists, we can easily transform it into a DataFrame and then export the DataFrame to a .csv file. For this last step, as we have a lot of text data, I recommended that you set the separator as \\\\t.'],\n",
       " ['p', 'This is how the data looks:'],\n",
       " ['p',\n",
       "  'As you can see, we have scraped data from 21,616 Medium articles. That’s a lot! That actually means our scraper accessed almost 22 thousand Medium pages.'],\n",
       " ['p',\n",
       "  'In fact, considering one archive page for each day of the year, we just accessed (21,616 + 365 =) 21,981 pages.'],\n",
       " ['p',\n",
       "  'This huge amount of requests we made can be a problem, though. The website we’re scraping can realize the interactions are not being made by a human and this can easily get our IP blocked.'],\n",
       " ['p',\n",
       "  'There are some workarounds we can do to fix this. One solution is to insert small pauses in your code, to make the interactions with the server more human. We can use the randint function from NumPy and the sleep function to achieve this:'],\n",
       " ['p',\n",
       "  'This code will randomly choose a number of seconds from 1 to 15 for the scraper to pause.'],\n",
       " ['p',\n",
       "  'But if you’re scraping too much data, even these pauses may not be enough. In this case, you could develop your own infrastructure of IP addresses. If you want to keep it simple, however, you could also get in touch with a proxy provider (such as Infatica or others) and they will deal with this problem for you by constantly changing your IP address so you do not get blocked.'],\n",
       " ['p',\n",
       "  'That’s something you might be asking yourself. Well, there’s always a lot to learn from data. We can perform some analysis to answer some simple questions, such as:'],\n",
       " ['oli', 'Is the number of articles in The Startup increasing over time?'],\n",
       " ['oli', 'What’s the average size of a story in The Startup?'],\n",
       " ['p',\n",
       "  'The charts below can help with those questions. Notice how the number of stories published per month skyrocketed in the second half of 2019. Also, the stories became around five paragraphs shorter, on average, throughout the year. And I’m talking paragraphs, but you could look for the average number of words or even characters per story.'],\n",
       " ['p',\n",
       "  'And of course, there’s Natural Language Processing\\u200a—\\u200aNLP. Yes, we have a lot of text data that we can use for NLP. It is possible to analyze the kind of stories that are usually published in The Startup, to investigate what makes a story receive more or fewer claps, or even to predict the number of claps and responses a new article may receive.'],\n",
       " ['p',\n",
       "  'Yes, there is a lot you can do with the data, but don’t miss the point here. This article is about the scraper, not the scraped data. The main goal here is to share how powerful of a tool it can be.'],\n",
       " ['p',\n",
       "  'Also, this same concept of web scraping can be used to perform a lot of different activities. For example, you can scrape Amazon a keep track of prices, or you can build a dataset of job opportunities by scraping a job search website if you are looking for a job.'],\n",
       " ['p', \"The possibilities are endless, so it's up to you!\"],\n",
       " ['p',\n",
       "  'If you liked this in think it may be useful to you, you can find the complete code here. If you have any questions or suggestions, feel free to get in touch.'],\n",
       " ['p',\n",
       "  'Also published on: https://datascienceplus.com/scraping-medium-publications-a-tutorial-for-beginners-with-python/'],\n",
       " ['p', 'Encode, Stream, and Manage Videos With One Simple Platform'],\n",
       " ['uli', 'Careers'],\n",
       " ['uli', 'Contact'],\n",
       " ['uli', 'Cookies'],\n",
       " ['uli', 'Emails'],\n",
       " ['uli', 'Help'],\n",
       " ['uli', 'Privacy'],\n",
       " ['uli', 'Terms'],\n",
       " ['uli', 'Archive'],\n",
       " ['uli', 'Leaderboard'],\n",
       " ['uli', 'Noonification'],\n",
       " ['uli', 'Signup'],\n",
       " ['uli', 'Tech Brief'],\n",
       " ['uli', 'Tech Tags'],\n",
       " ['uli', 'Top Stories'],\n",
       " ['uli', 'Distribution'],\n",
       " ['uli', 'Editor Tips'],\n",
       " ['uli', 'Guidelines'],\n",
       " ['uli', 'New Story'],\n",
       " ['uli', 'Perks'],\n",
       " ['uli', 'Prompts'],\n",
       " ['uli', 'Why Write'],\n",
       " ['uli', 'Billboard'],\n",
       " ['uli', 'Brand Publishing'],\n",
       " ['uli', 'Case Studies'],\n",
       " ['uli', 'Contests'],\n",
       " ['uli', 'Niche Marketing'],\n",
       " ['uli', 'Newsletter'],\n",
       " ['uli', 'Writing Contests'],\n",
       " ['p', 'Quality Weekly Reads About Technology Infiltrating Everything']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the rules stated below shouldnt be coded, as they take away flexibility and scrape insufficient information\n",
    "\n",
    "# empty headings\n",
    "# headings without paragraphs\n",
    "# headings with paragraphs connected to headinge without paragraphs\n",
    "# headings that aren't connected to the main topic at all\n",
    "\n",
    "# coinbase throws error\n",
    "\n",
    "# text has '\\n'\n",
    "\n",
    "# multiple h1 in the start\n",
    "# h1 at the end\n",
    "\n",
    "# paragraphs that have \\t or \\n only, same with headings\n",
    "\n",
    "# potential links to other articles in the headings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '  \\n\\n\\n\\n    Blockchain resources\\n\\n\\n\\n\\n\\n    \\n\\n\\t\\n            '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ''.join([a[i] for i in range(len(a)) if a[i:i+1]!='\\n'])\n",
    "a = ''.join([a[i] for i in range(len(a)) if a[i:i+1]!='\\t'])\n",
    "a = a.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blockchain resources'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      Blockchain resources                '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanspaces('      Blockchain resources                ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '      Blockchain resources                '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Blockchain',\n",
       " 'resources',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
